{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Denoising: Total Variation Regularization\n",
    "\n",
    "The problem of removing noise from an image without blurring\n",
    "sharp edges can be formulated as an infinite-dimensional minimization\n",
    "problem. Given a possibly noisy image $d(x,y)$ defined within a\n",
    "rectangular domain $\\Omega$, we would like to\n",
    "find the image $m(x,y)$ that is closest in the $L_2$ sense, i.e. we\n",
    "want to minimize \n",
    "\n",
    "$$ \\mathcal{J}_{LS} := \\frac{1}{2}\\int_\\Omega (m - d)^2 \\; d\\boldsymbol{x}, $$\n",
    "\n",
    "\n",
    "while also removing noise, which is assumed to comprise very *rough*\n",
    "components of the image. This latter goal can be incorporated as an\n",
    "additional term in the objective, in the form of a penalty. \n",
    "\n",
    "As we observed in the *Image Denoising: Tikhonov regularization* notebook,\n",
    "the Tikhonov regularization functional,\n",
    "$$ \\mathcal{R}_{TN} := \\! \\frac{\\alpha}{2}\\int_\\Omega \\nabla m\n",
    "\\cdot \\! \\nabla m \\; d\\boldsymbol{x}, $$\n",
    "has the tendency of blurring sharp edges in the image.\n",
    "\n",
    "Instead,\n",
    "in these cases we prefer the so-called *total variation (TV)\n",
    "regularization*,\n",
    "\n",
    "$$ \\mathcal{R}_{TV} := \\! \\alpha\\int_\\Omega (\\nabla m \\cdot \\! \\nabla\n",
    "m)^{\\frac{1}{2}} \\; d\\boldsymbol{x} $$\n",
    "\n",
    "where (we will see that) taking the square root is the key to\n",
    "preserving edges. Since \n",
    "$\\mathcal{R}_{TV}$ is not differentiable when $\\nabla m =\n",
    "\\boldsymbol{0}$, it is usually modified to include a positive parameter\n",
    "$\\beta$ as follows:\n",
    "\n",
    "$$ \\mathcal{R}^{\\beta}_{TV} := \\!  \\alpha \\int_\\Omega |\\nabla m|_\\beta \\; d\\boldsymbol{x}, \\quad \\text{where } |\\nabla m|_\\beta =(\\nabla m \\cdot\n",
    "\\! \\nabla m + \\beta)^{\\frac{1}{2}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Python imports\n",
    "\n",
    "We import the following libraries:\n",
    "\n",
    "- `math`, which contains several mathematical functions\n",
    "- `matplotlib, numpy, scipy`, three libraries that together allow similar functionalities to matlab\n",
    "- `dolfin`, which allows us to discretize and solve variational problems using the finite element method\n",
    "- `hippylib`, the extesible framework I created to solve inverse problems in Python\n",
    "- `unconstrainedMinimization`, which has a Python implementation of the inexact Newton Conjuge Gradient algorithm that used in Assignment 3\n",
    "\n",
    "Finally, we import the `logging` library to silence most of the output produced by `dolfin`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "\n",
    "import dolfin as dl\n",
    "\n",
    "import hippylib as hp\n",
    "from hippylib import nb\n",
    "\n",
    "from unconstrainedMinimization import InexactNewtonCG\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.getLogger('FFC').setLevel(logging.WARNING)\n",
    "logging.getLogger('UFL').setLevel(logging.WARNING)\n",
    "dl.set_log_active(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Geometry, true image, and data.\n",
    "\n",
    "1. Read the true image from file, store the pixel values in `data`\n",
    "\n",
    "2. The width of the image is `Lx = 1` and height `Ly` is set such that the aspect ratio of the image is preserved.\n",
    "\n",
    "3. Generate a triangulation (pixelation) `mesh` of the region of interest.\n",
    "\n",
    "4. Define the finite element space `V` of piecewise linear function on the elements of `mesh`. This represent the space of discretized images.\n",
    "\n",
    "5. Interpolate the true image in the discrete space `V`. Call this interpolation `m_true`.\n",
    "\n",
    "6. Corrupt the true image with i.i.d. Gaussian noise ($\\sigma^2 = 0.09$) and interpolated the noisy image in the discrete space `V`. Call this interpolation `d`.\n",
    "\n",
    "7. Visualize the true image `m_true` and the noisy image `d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sio.loadmat('circles.mat')['im']\n",
    "\n",
    "Lx = 1.\n",
    "h = Lx/float(data.shape[0])\n",
    "Ly = float(data.shape[1])*h\n",
    "  \n",
    "mesh = dl.RectangleMesh(dl.Point(0,0),dl.Point(Lx,Ly),data.shape[0], data.shape[1])\n",
    "V = dl.FunctionSpace(mesh, \"Lagrange\",1)\n",
    "\n",
    "trueImage = hp.NumpyScalarExpression2D()\n",
    "trueImage.setData(data, h, h)\n",
    "m_true  = dl.interpolate(trueImage, V)\n",
    "\n",
    "np.random.seed(seed=1)\n",
    "noise_std_dev = .3\n",
    "noise = noise_std_dev*np.random.randn(data.shape[0], data.shape[1])\n",
    "noisyImage = hp.NumpyScalarExpression2D()\n",
    "noisyImage.setData(data+noise, h, h)\n",
    "d = dl.interpolate(noisyImage, V)\n",
    "\n",
    "# Get min/max of noisy image, so that we can show all plots in the same scale\n",
    "vmin = np.min(d.vector().get_local())\n",
    "vmax = np.max(d.vector().get_local())\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "nb.plot(m_true, subplot_loc=121, mytitle=\"True Image\", vmin=vmin, vmax = vmax)\n",
    "nb.plot(d, subplot_loc=122, mytitle=\"Noisy Image\", vmin=vmin, vmax = vmax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Total Variation denoising\n",
    "\n",
    "The class `TVDenosing` defines the cost functional and its first & second variations for the Total Variation denoising problem.\n",
    "\n",
    "Specifically, the cost functional reads\n",
    "$$ \\mathcal{J}(m) = \\frac{1}{2}\\int_\\Omega (m - d)^2 \\; d\\boldsymbol{x} + \\alpha\\int_\\Omega |\\nabla m|_\\beta d\\boldsymbol{x}, $$\n",
    "where $\\alpha$ is the amount of regularization and $\\beta$ is a small pertubation to ensure differentiability of the total variation functional.\n",
    "\n",
    "The first variation of $\\mathcal{J}$ reads\n",
    "$$ \\delta_m \\mathcal{J}(m, \\tilde{m}) = \\int_\\Omega (m - d)\\tilde{m}  \\; d\\boldsymbol{x} + \\alpha \\int_\\Omega \\frac{1}{|\\nabla m|_\\beta}\\nabla m \\cdot \\nabla \\tilde{m}  d\\boldsymbol{x}, $$\n",
    "and the second variation is\n",
    "$$ \\delta_m^2 \\mathcal{J}(m, \\tilde{m}, \\hat{m}) = \\int_\\Omega \\tilde{m} \\hat{m} \\; d\\boldsymbol{x} + \\alpha \\int_\\Omega \\frac{1}{|\\nabla m|_\\beta} \\left[ \\left( I - \\frac{\\nabla m \\otimes \\nabla m}{|\\nabla m|^2_\\beta}\\right) \\nabla \\tilde{m}\\right] \\cdot \\nabla \\hat{m} d\\boldsymbol{x}. $$\n",
    "\n",
    "The highly nonlinear coefficient $A = \\left( I - \\frac{\\nabla m \\otimes \\nabla m}{|\\nabla m|^2_\\beta}\\right) $ in the second variation poses a substantial challange for the convergence of the Newton's method. In fact, the converge radius of the Newtos's method is extremely small.\n",
    "For this reason in the following we will replace the second variation with the variational form\n",
    "$$ \\delta_m^2 \\mathcal{J}_{\\rm approx}(m, \\tilde{m}, \\hat{m}) = \\int_\\Omega \\tilde{m}\\,\\hat{m} \\; d\\boldsymbol{x} + \\alpha \\int_\\Omega \\frac{1}{|\\nabla m|_\\beta} \\nabla \\tilde{m} \\cdot \\nabla \\hat{m} d\\boldsymbol{x}. $$\n",
    "The resulting method will exhibit only a first order convergence rate but it will be more robust for small values of $\\beta$.\n",
    "\n",
    "> For small values of $\\beta$, there are more efficient methods for solving TV-regularized inverse problems than the basic Newton method we use here; in particular, so-called primal-dual Newton methods are preferred (see T.F. Chan, G.H. Golub, and P. Mulet, *A nonlinear primal-dual method for total variation-based image restoration*, SIAM Journal on Scientific Computing, 20(6):1964–1977, 1999)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TVDenoising:\n",
    "    def __init__(self, V, d, alpha, beta):\n",
    "        self.alpha   = dl.Constant(alpha)\n",
    "        self.beta    = dl.Constant(beta)\n",
    "        self.d       = d\n",
    "        self.m_tilde  = dl.TestFunction(V)\n",
    "        self.m_hat = dl.TrialFunction(V)\n",
    "        \n",
    "    def cost_reg(self, m):\n",
    "        return dl.sqrt( dl.inner(dl.grad(m), dl.grad(m)) + self.beta)*dl.dx\n",
    "    \n",
    "    def cost_misfit(self, m):\n",
    "        return dl.Constant(.5)*dl.inner(m-self.d, m - self.d)*dl.dx\n",
    "        \n",
    "    def cost(self, m):        \n",
    "        return self.cost_misfit(m) + self.alpha*self.cost_reg(m)\n",
    "        \n",
    "    def grad(self, m):    \n",
    "        grad_ls = dl.inner(self.m_tilde, m - self.d)*dl.dx\n",
    "        \n",
    "        TVm = dl.sqrt( dl.inner(dl.grad(m), dl.grad(m)) + self.beta)\n",
    "        grad_tv = dl.Constant(1.)/TVm*dl.inner(dl.grad(m), dl.grad(self.m_tilde))*dl.dx\n",
    "        \n",
    "        grad = grad_ls + self.alpha*grad_tv\n",
    "        \n",
    "        return grad\n",
    "        \n",
    "    def Hessian(self,m):\n",
    "        H_ls = dl.inner(self.m_tilde, self.m_hat)*dl.dx\n",
    "        \n",
    "        TVm = dl.sqrt( dl.inner(dl.grad(m), dl.grad(m)) + self.beta)\n",
    "        A = dl.Constant(1.)/TVm * (dl.Identity(2) - dl.outer(dl.grad(m)/TVm, dl.grad(m)/TVm ) )\n",
    "        H_tv = dl.inner(A*dl.grad(self.m_tilde), dl.grad(self.m_hat))*dl.dx\n",
    "         \n",
    "        H = H_ls + self.alpha*H_tv\n",
    "                                   \n",
    "        return H\n",
    "    \n",
    "    def LD_Hessian(self,m):\n",
    "        H_ls = dl.inner(self.m_tilde, self.m_hat)*dl.dx\n",
    "        \n",
    "        TVm = dl.sqrt( dl.inner(dl.grad(m), dl.grad(m)) + self.beta)\n",
    "        H_tv = dl.Constant(1.)/TVm *dl.inner(dl.grad(self.m_tilde), dl.grad(self.m_hat))*dl.dx\n",
    "         \n",
    "        H = H_ls + self.alpha*H_tv\n",
    "                                   \n",
    "        return H\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Finite difference check of the gradient\n",
    "\n",
    "We use a *finite difference check* to verify that our implementation of the gradient is correct. \n",
    "\n",
    "Specifically, we consider an arbitrary chosen function $m_0=x(x−1)y(y−1)$ and we verify that for a random direction $\\tilde{m}$ we have\n",
    "\n",
    "$$\n",
    "r:=\\left| \\frac{ \\mathcal{J}(m_0 + \\varepsilon \\tilde{m})−\\mathcal{J}(m_0) }{h} - \\delta_m \\mathcal{J}(m_0, \\tilde{m})\\right|=\\mathcal{O}(\\varepsilon).\n",
    "$$\n",
    "\n",
    "In the figure below we plot in a loglog scale the value of $r$ as a function of $\\varepsilon$. We observe that $r$ decays linearly for a wide range of values of $\\varepsilon$, however we notice an increase in the error for extremely small values of $\\varepsilon$ due to numerical stability and finite precision arithmetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_eps = 32\n",
    "eps = 1e-2*np.power(2., -np.arange(n_eps))\n",
    "err_grad = np.zeros(n_eps)\n",
    "\n",
    "m0 = dl.interpolate(dl.Expression(\"x[0]*(x[0]-1)*x[1]*(x[1]-1)\", degree=4), V)\n",
    "alpha = 1.\n",
    "beta   = 1e-4\n",
    "problem = TVDenoising(V,d,alpha, beta)\n",
    "\n",
    "J0 = dl.assemble( problem.cost(m0) )\n",
    "grad0 = dl.assemble(problem.grad(m0) )\n",
    "\n",
    "mtilde = dl.Function(V)\n",
    "mtilde.vector().set_local(np.random.randn(V.dim()))\n",
    "mtilde.vector().apply(\"\")\n",
    "\n",
    "mtilde_grad0 = grad0.inner(mtilde.vector())\n",
    "\n",
    "for i in range(n_eps):\n",
    "    Jplus = dl.assemble( problem.cost(m0 + dl.Constant(eps[i])*mtilde) )\n",
    "    err_grad[i] = abs( (Jplus - J0)/eps[i] - mtilde_grad0 )\n",
    "\n",
    "plt.figure()    \n",
    "plt.loglog(eps, err_grad, \"-ob\", label=\"Error Grad\")\n",
    "plt.loglog(eps, (.5*err_grad[0]/eps[0])*eps, \"-.k\", label=\"First Order\")\n",
    "plt.title(\"Finite difference check of the first variation (gradient)\")\n",
    "plt.xlabel(\"eps\")\n",
    "plt.ylabel(\"Error grad\")\n",
    "plt.legend(loc = \"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Finite difference check of the Hessian\n",
    "\n",
    "As before, we verify that for a random direction $\\hat{m}$ we have \n",
    "\n",
    "$$\n",
    "r:=\\left\\| \\frac{\\delta_m\\mathcal{J}(m_0+\\varepsilon\\hat{m},\\tilde{m} ) - \\delta_m\\mathcal{J}(m_0,\\tilde{m} )}{\\varepsilon} - \\delta^2_m\\mathcal{J}(m_0,\\tilde{m}, \\hat{m})\\right\\| = \\mathcal{O}(\\varepsilon).\n",
    "$$\n",
    "\n",
    "In the figure below we show in a loglog scale the value of $r$\n",
    "as a function of $\\varepsilon$. As before, we observe that $r$ decays linearly for a wide range of values of $\\varepsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_0 = dl.assemble( problem.Hessian(m0) )\n",
    "H_0mtilde = H_0 * mtilde.vector()\n",
    "err_H = np.zeros(n_eps)\n",
    "\n",
    "for i in range(n_eps):\n",
    "    grad_plus = dl.assemble( problem.grad(m0 + dl.Constant(eps[i])*mtilde) )\n",
    "    diff_grad = (grad_plus - grad0)\n",
    "    diff_grad *= 1/eps[i]\n",
    "    err_H[i] = (diff_grad - H_0mtilde).norm(\"l2\")\n",
    "    \n",
    "plt.figure()    \n",
    "plt.loglog(eps, err_H, \"-ob\", label=\"Error Hessian\")\n",
    "plt.loglog(eps, (.5*err_H[0]/eps[0])*eps, \"-.k\", label=\"First Order\")\n",
    "plt.title(\"Finite difference check of the second variation (Hessian)\")\n",
    "plt.xlabel(\"eps\")\n",
    "plt.ylabel(\"Error Hessian\")\n",
    "plt.legend(loc = \"upper left\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Solution of Total Variation regularized Denoising problem.\n",
    "\n",
    "The function `TVsolution` computes the solution of the denoising inverse problem using Total Variation regularization for a given amount a regularization $\\alpha$ and perturbation $\\varepsilon$.\n",
    "\n",
    "To minimize the cost cost functional $\\mathcal{J}(m)$, we use the *infinite-dimensional* Newton's method with linesearch globalization.\n",
    "\n",
    "1. Given the current solution $m_k$, find the Newton's direction $\\hat{m}_k$ such that\n",
    "\n",
    "$$ \\delta^2_m\\mathcal{J}(m_k,\\tilde{m}, \\hat{m}_k )=−\\delta_m\\mathcal{J}(m_k,\\tilde{m}) \\quad \\forall \\tilde{m}.$$\n",
    "\n",
    "2. Update the solution using the Newton direction $\\hat{m}_k$\n",
    "\n",
    "$$ m_{k+1}=m_k+ \\alpha_k \\hat{m}_k, $$\n",
    "\n",
    "where the step length $\\alpha_k$ is computed using backtracking to the ensure sufficient descent condition.\n",
    "\n",
    "The class `InexactNewtonCG` implements the inexact Newton conjugate gradient algorithm to solve the discretized version of the *infinite-dimensional* Newton's method above.\n",
    "\n",
    "Finally, since we also know the true image `m_true` (*this will not be the case for real applications*)\n",
    "we can also write the true $L_2$ error functional \n",
    "$$ MSE := \\frac{1}{2}\\int_\\Omega (m - m_{\\rm true})^2 \\; d\\boldsymbol{x}. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TVsolution(alpha, beta):\n",
    "    \n",
    "    m = dl.Function(V)\n",
    "    problem = TVDenoising(V, d, alpha, beta)\n",
    "    \n",
    "    solver = InexactNewtonCG()\n",
    "    solver.parameters[\"rel_tolerance\"] = 1e-5\n",
    "    solver.parameters[\"abs_tolerance\"] = 1e-9\n",
    "    solver.parameters[\"gdm_tolerance\"] = 1e-18\n",
    "    solver.parameters[\"max_iter\"] = 1000\n",
    "    solver.parameters[\"c_armijo\"] = 1e-5\n",
    "    solver.parameters[\"print_level\"] = -1\n",
    "    solver.parameters[\"max_backtracking_iter\"] = 10\n",
    "    solver.solve(problem.cost, problem.grad, problem.Hessian, m)\n",
    "    \n",
    "    MSE  = dl.inner(m - m_true, m - m_true)*dl.dx\n",
    "    J    = problem.cost(m)\n",
    "    J_ls = problem.cost_misfit(m)\n",
    "    R_tv = problem.cost_reg(m)\n",
    "\n",
    "    print( \"{0:15e} {1:15e} {2:4d} {3:15e} {4:15e} {5:15e} {6:15e}\".format(\n",
    "           alpha, beta, solver.it, dl.assemble(J), dl.assemble(J_ls), dl.assemble(R_tv), dl.assemble(MSE))\n",
    "         )\n",
    "\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compute the denoised image for different amout of regularization\n",
    "\n",
    "We define some values of $\\alpha$ ($\\alpha = 10^{-1}, 10^{-2}, 10^{-3}, 10^{-4}$) for the choice of the regularization paramenter.\n",
    "\n",
    "The best reconstruction of the original image is obtained for $\\alpha = 10^{-3}$. We also notice that Total Variation does a much better job that Tikhonov regularization in preserving the sharp edges of the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"{0:15} {1:15} {2:4} {3:15} {4:15} {5:15} {6:15}\".format(\"alpha\", \"beta\", \"nit\", \"J\", \"J_ls\", \"R_tv\", \"MSE\") )\n",
    "\n",
    "beta = 1e-4\n",
    "n_alphas = 4\n",
    "alphas = np.power(10., -np.arange(1, n_alphas+1))\n",
    "for alpha in alphas:\n",
    "    m = TVsolution(alpha, beta)\n",
    "    plt.figure()\n",
    "    nb.plot(m, vmin=vmin, vmax = vmax, mytitle=\"alpha = {0:1.2e}\".format(alpha))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Copyright &copy; 2019-2020, Washington University in St. Louis.\n",
    "\n",
    "All Rights reserved.\n",
    "See file COPYRIGHT for details.\n",
    "\n",
    "This file is part of **cmis_labs**, the teaching material for  ESE 5932 *Computational Methods for Imaging Science* at Washington University in St. Louis. Please see [https://uvilla.github.io/cmis_labs](https://uvilla.github.io/cmis_labs) for more information and source code availability.\n",
    "\n",
    "We would like to acknowledge the Extreme Science and Engineering Discovery Environment (XSEDE), which is supported by National Science Foundation grant number ACI-1548562, for providing cloud computing resources (Jetstream) for this course through allocation TG-SEE190001."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
