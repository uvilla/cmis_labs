{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectrum of the preconditioned Hessian misfit operator\n",
    "## The linear source inversion problem\n",
    "\n",
    "We consider the following linear source inversion problem.\n",
    "Find the state $u \\in \\mathcal{V} := H^1_{\\Gamma_D}(\\Omega)$ and the source (*parameter*) $m \\in \\mathcal{M} := H^1(\\Omega)$ that solves\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "{} & \\min_m \\underbrace{\\frac{1}{2} \\| Bu - \\boldsymbol{d} \\|^2 + \\frac{1}{2} \\int_\\Omega \\left[ \\delta|m-m_0|^2 + \\gamma|\\nabla (m - m_0)|^2 \\right] dx}_{J(u(m), m)} & {}\\\\\n",
    "{\\rm s.t.} & {} &{} \\\\\n",
    "{} & -{\\rm div}(k \\nabla u) + cu = m & {\\rm in} \\; \\Omega\\\\\n",
    "{} & u = 0 & {\\rm on } \\; \\Gamma_D\\\\\n",
    "{} & k \\frac{\\partial u}{\\partial n} = 0 & {\\rm on } \\; \\Gamma_N\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Here:\n",
    "\n",
    "- $\\boldsymbol{d}$ is a $n_{\\rm obs}$ finite dimensional vector that denotes noisy observations of the state $u$ in $n_{\\rm obs}$ locations $\\mathbf{x}_i$, $i=1,\\ldots,n_{\\rm obs}$. Specifically, $d_i = u_{\\rm true}( {\\bf x}_i ) + \\eta_i$, where $\\eta_i$ are i.i.d. $\\mathcal{N}(0, \\sigma^2)$.\n",
    "\n",
    "- $B: \\mathcal{V} \\rightarrow \\mathbb{R}^{n_{\\rm obs}}$ is the linear operator that evaluates the state $u$ at the observation locations $\\mathbf{x}_i$, $i=1,\\ldots,n_{\\rm obs}$.\n",
    "\n",
    "- $\\| \\cdot \\|$ denotes the Euclidean norm in $\\mathbb{R}^{n_{\\rm obs}}$.\n",
    "\n",
    "- $\\delta$ and $\\gamma$ are the parameters of the regularization penalizing the $L^2(\\Omega)$ and $H^1(\\Omega)$ norm of $m-m_0$, respectively.\n",
    "\n",
    "- $k$ and $c$ are given coefficients representing the diffusivity coefficient and the reaction term, respectively.\n",
    "\n",
    "- $\\Gamma_D \\subset \\partial \\Omega$, $\\Gamma_N \\subset \\partial \\Omega$ represents the subdomain of $\\partial\\Omega$ where we impose Dirichlet or Neumann boundary conditions, respectively.\n",
    "\n",
    "### The variational (or weak) form of the forward problem:\n",
    "\n",
    "Find $u\\in \\mathcal{V}$ such that \n",
    "\n",
    "$$ \\int_{\\Omega} k \\nabla u \\cdot \\nabla \\tilde{p} \\, dx +\\int_{\\Omega} c\\, u\\,\\tilde{p} \\, dx  = \\int_{\\Omega}  m\\,\\tilde{p} \\, dx, \\text{ for all } \\tilde{p} \\in \\mathcal{V}.$$\n",
    "\n",
    "### Gradient evaluation:\n",
    "\n",
    "The Lagrangian functional $\\mathscr{L}: \\mathcal{V} \\times \\mathcal{M} \\times \\mathcal{V} \\rightarrow \\mathbb{R}$ is given by\n",
    "\n",
    "$$\n",
    "\\mathscr{L}(u,m,p):= \\underbrace{\\frac{1}{2} \\| Bu - \\boldsymbol{d} \\|^2 + \\frac{1}{2} \\int_\\Omega \\left[ \\delta|m-m_0|^2 + \\gamma|\\nabla (m - m_0)|^2 \\right] dx}_{J(u,m)} +  \\underbrace{\\int_{\\Omega} k \\nabla u \\cdot \\nabla p \\, dx +\\int_{\\Omega} c\\, u\\,p \\, dx  - \\int_{\\Omega}  m\\,p \\, dx}_{\\text{forward problem}}.\n",
    "$$\n",
    "\n",
    "Then the gradient of the cost functional $\\mathcal{J}(u(m), m)$ with respect to the parameter $m$ is\n",
    "\n",
    "$$\n",
    "    (\\mathcal{G}(m), \\tilde m) := \\mathscr{L}_m(u,m,p)(\\tilde{m}) = \\gamma \\int_\\Omega \\nabla (m-m_0) \\cdot \\nabla \\tilde{m}\\, dx + \\delta \\int_{\\Omega} (m-m_0) \\cdot \\tilde{m} \\, dx -\n",
    "     \\int_\\Omega \\tilde{m}\\, p\\, dx \\quad \\forall \\tilde{m} \\in \\mathcal{M},\n",
    "$$\n",
    "\n",
    "where $u \\in \\mathcal{V}$ is the solution of the forward problem,\n",
    "\n",
    "$$ (\\mathscr{L}_p(u,m,p), \\tilde{p}) := \\int_{\\Omega} k \\nabla u \\cdot \\nabla \\tilde{p} \\, dx +\\int_{\\Omega} c\\, u\\,\\tilde{p} \\, dx  - \\int_{\\Omega}  m\\,\\tilde{p} \\, dx =  0\n",
    "\\quad \\forall \\tilde{p} \\in \\mathcal{V}, $$\n",
    "\n",
    "and $p \\in \\mathcal{V}$ is the solution of the adjoint problem,\n",
    "\n",
    "$$ (\\mathscr{L}_u(u,m,p), \\tilde{u}) := \\int_{\\Omega} k \\nabla \\tilde{u} \\cdot \\nabla p \\, dx +\\int_{\\Omega} c\\, \\tilde{u} \\,p \\, dx + \\int_\\Omega \\tilde{u}B^*(Bu - \\boldsymbol{d})\\,dx = 0\n",
    "\\quad \\forall \\tilde{u} \\in \\mathcal{V}.$$\n",
    "\n",
    "Above $B^*: \\mathbb{R}^{n_{\\rm obs}} \\mapsto \\mathcal{V}$ is the adjoint of $B$, i.e. the operator defined as\n",
    "\n",
    "$$ \\underbrace{y^T (B u)}_{\\text{inner product in } \\mathbb{R}^{n_{\\rm obs}}} = \\underbrace{\\int_\\Omega u\\, B^*y \\, dx}_{\\text{inner product  in }\\mathcal{V}} \\quad forall y \\in \\mathbb{R}^{n_{\\rm obs}},u \\in \\mathcal{V}. $$\n",
    "\n",
    "### Hessian action:\n",
    "\n",
    "To evaluate the action $\\mathcal{H}(m)(\\hat{m})$ of the Hessian is a given direction $\\hat{m}$, we consider variations of the meta-Lagrangian functional\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathscr{L}^H(u,m,p; \\hat{u}, \\hat{m}, \\hat{p}) := & {} & {} \\\\\n",
    "{} & \\gamma \\int_\\Omega \\nabla (m-m_0) \\cdot \\nabla \\hat{m}\\, dx + \\delta \\int_{\\Omega} (m-m_0) \\cdot \\hat{m} \\, dx -\n",
    "     \\int_\\Omega \\hat{m}\\, p\\, dx & \\text{gradient}\\\\\n",
    "{} & + \\int_{\\Omega} k \\nabla u \\cdot \\nabla \\hat{p} \\, dx +\\int_{\\Omega} c\\, u\\,\\hat{p} \\, dx  - \\int_{\\Omega}  m\\,\\hat{p} \\, dx & \\text{forward eq}\\\\\n",
    "{} & + \\int_{\\Omega} k \\nabla \\hat{u} \\cdot \\nabla p \\, dx +\\int_{\\Omega} c\\, \\hat{u} \\,p \\, dx + \\int_\\Omega \\hat{u}\\,B^*(Bu - \\boldsymbol{d})\\,dx & \\text{adjoint eq}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Then action of the Hessian is a given direction $\\hat{m}$ is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "(\\tilde{m}, \\mathcal{H}(m)(\\hat{m}) ) & := \\mathscr{L}^H_m(u,m,p; \\hat{u}, \\hat{m}, \\hat{p})(\\tilde{m}) \\\\\n",
    "{} & =\n",
    "\\gamma \\int_\\Omega \\nabla \\tilde{m}\\cdot \\nabla \\hat{m}\\, dx + \\delta \\int_{\\Omega} \\tilde{m}\\cdot \\hat{m} \\, dx \n",
    "- \\int_{\\Omega}  \\tilde{m} \\,\\hat{p} \\, dx\n",
    "\\quad \\forall \\tilde{m} \\in \\mathcal{M},\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "- $u\\in \\mathcal{V}$ and $p \\in \\mathcal{V}$ are the solution of the forward and adjoint problem, respectively;\n",
    "\n",
    "- $\\hat{u} \\in \\mathcal{V}$ is the solution of the incremental forward problem,\n",
    "\n",
    "$$\n",
    "\\left( \\mathscr{L}^H_p(u,m,p; \\hat{u}, \\hat{m}, \\hat{p}),\\tilde{p}\\right) := \\int_{\\Omega} k \\nabla \\hat{u} \\cdot \\nabla \\tilde{p} \\, dx +\\int_{\\Omega} c\\, \\hat{u}\\,\\tilde{p} \\, dx  - \\int_{\\Omega}  \\hat{m}\\,\\tilde{p} \\, dx =  0 \\quad \\forall \\tilde{p} \\in \\mathcal{V};\n",
    "$$\n",
    "\n",
    "\n",
    "- and $\\hat{p} \\in \\mathcal{V}$ is the solution of the incremental adjoint problem,\n",
    "$$\n",
    "\\left( \\mathscr{L}^H_u(u,m,p; \\hat{u}, \\hat{m}, \\hat{p}), \\tilde{u}\\right) := \\int_{\\Omega} k \\nabla \\tilde{u} \\cdot \\nabla \\hat{p} \\, dx +\\int_{\\Omega} c\\, \\tilde{u} \\,\\hat{p} \\, dx + \\int_\\Omega \\tilde{u}B^*B\\hat{u}\\,dx = 0 \\quad \\forall \\tilde{u} \\in \\mathcal{V}.\n",
    "$$\n",
    "\n",
    "It worth to notice that the Hessian expression does not depend on the data $u_d$ or on $u$, $m$, $p$. This is a characteristic of all linear inverse problems.\n",
    "\n",
    "### Solution of inverse problem\n",
    "\n",
    "Note that due to linearity of the inverse problem, Newton's method will converge in a single iteration.\n",
    "That is the solution of the inverse problem $m^*$ is the solution of the linear system\n",
    "$$\n",
    "\\left(\\tilde{m}, \\mathcal{H}\\,m^* \\right) = -\\left(\\tilde{m}, \\mathcal{G}(0)\\right) \\quad \\forall \\tilde{m} \\in mathcal{M},\n",
    "$$\n",
    "\n",
    "where the evaluation of the gradient $\\mathcal{G}(0)$ involve the solution $u$ and $p$ of the forward and adjoint problem (respectively) for $m = 0$.\n",
    "Similarly, the Hessian action in an arbitrary direction $\\hat{m}$ requires to additional solve the incremental forward and adjoint problems.\n",
    "\n",
    "### Discrete optimality conditions:\n",
    "$\n",
    "\\def\\tu{\\tilde u}\n",
    "\\def\\tm{\\tilde m}\n",
    "\\def\\tp{\\tilde p}\n",
    "\\def\\hu{\\hat u}\n",
    "\\def\\hp{\\hat p}\n",
    "\\def\\hm{\\hat m}\n",
    "$\n",
    "$\n",
    "\\def\\bu{{\\bf u}}\n",
    "\\def\\bm{{\\bf m}}\n",
    "\\def\\bp{{\\bf p}}\n",
    "\\def\\btu{{\\bf \\tilde u}}\n",
    "\\def\\btm{{\\bf \\tilde m}}\n",
    "\\def\\btp{{\\bf \\tilde p}}\n",
    "\\def\\bhu{{\\bf \\hat u}}\n",
    "\\def\\bhm{{\\bf \\hat m}}\n",
    "\\def\\bhp{{\\bf \\hat p}}\n",
    "\\def\\bg{{\\bf g}}\n",
    "$\n",
    "$\n",
    "\\def\\bA{{\\bf A}}\n",
    "\\def\\bB{{\\bf B}}\n",
    "\\def\\bC{{\\bf C}}\n",
    "\\def\\bH{{\\bf H}}\n",
    "\\def\\bR{{\\bf R}}\n",
    "\\def\\bW{{\\bf W}}\n",
    "$\n",
    "\n",
    "Let us introduce the finite dimensional subspace $\\mathcal{M}_h \\subset \\mathcal{M}$ and $\\mathcal{V}_h \\subset \\mathcal{V}$. Let $\\{ \\phi_i(\\boldsymbol{x}) \\}_{i=1}^{n_M}$ be a basis of $\\mathcal{M}_h$, and $\\{ \\psi_i(\\boldsymbol{x}) \\}_{i=1}^{n_V}$ be a basis of $\\mathcal{V}_h$.\n",
    "\n",
    "Define the following matrices: \n",
    "\n",
    "- $\\bA \\in \\mathbb{R}^{n_V \\times n_V}$ stemming from Galerkin discretization of the left hand side of the forward problem, i.e. whose (i,j)-entries is given by \n",
    "$$ A_{ij} = \\int_{\\Omega} k \\nabla \\psi_j \\cdot \\nabla \\psi_i \\, dx +\\int_{\\Omega} c\\, \\psi_j \\,\\psi_i \\, dx \\quad \\forall i,j=1,\\ldots, n_V;$$\n",
    "\n",
    "- $\\bC \\in \\mathbb{R}^{n_V \\times n_M}$ stemming from Galerkin discretization of the right hand side of the forward problem, i.e. whose (i,j)-entries is given by \n",
    "\n",
    "$$ C_{ij} = \\int_{\\Omega}  \\phi_j \\,\\psi_i \\, dx \\quad \\forall i=1,\\ldots, n_V, \\forall j=1,\\ldots, n_M$$\n",
    "\n",
    "- $\\bB \\in \\mathbb{R}^{n_{\\rm obs} \\times n_V}$ stemming from the discretization of the observation operator $B$\n",
    "\n",
    "- $\\bR$ stemming from Galerkin discretization of the Hessian of the regularization term, i.e. whose (i,j)-entries is given by\n",
    "$$ R_{ij} = \\gamma \\int_\\Omega \\nabla \\phi_j \\cdot \\nabla \\phi_i \\, dx + \\delta \\int_{\\Omega} \\phi_j \\, \\phi_i \\, dx $$\n",
    "\n",
    "Let us also denote the vectors corresponding to the discretization of the functions $u, m, p$ by $\\bu, \\bm, \\bp$.\n",
    "\n",
    "Then, using the above notation, we can rewrite the inverse problem as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "{} & \\min_\\bm \\frac{1}{2} \\| \\bB\\,\\bu - \\boldsymbol{d} \\|^2 + \\frac{1}{2} (\\bm-\\bm_0)^T \\bR (\\bm-\\bm_0) & {}\\\\\n",
    "{\\rm s.t.} & {} &{} \\\\\n",
    "{} & \\bA\\,\\bu = \\bC \\bm,\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $\\bm_0$ is the vector of degree of freedoms of the interpolant of $m_0$ onto $\\mathcal{M}_h$.\n",
    "\n",
    "The optimalitity conditions for the above discrete system can be readily obtained by eliminating the state $\\bu = \\bA^{-1}\\bC \\bm$.\n",
    "\n",
    "In fact, first order optimality conditions for the mimization problem  \n",
    "$$\n",
    "\\min_\\bm \\frac{1}{2} \\| \\bB\\bA^{-1}\\bC \\bm - \\boldsymbol{d} \\|^2 + \\frac{1}{2} (\\bm-\\bm_0)^T \\bR (\\bm-\\bm_0)\n",
    "$$\n",
    "are\n",
    "$$ \\underbrace{(\\bC^T \\bA^{-T}\\bB^T\\bB\\bA^{-1}\\bC +\\bR)}_{\\text{Hessian}} \\bm = \\underbrace{\\bC^T \\bA^{-T}\\bB^T\\boldsymbol{d} + \\bR \\bm_0}_{\\text{gradient at }\\bm = \\boldsymbol{0} }.$$\n",
    "\n",
    "Note that:\n",
    "1) Computing the gradient at $\\bm = \\boldsymbol{0}$ requires solving\n",
    "- the forward problem $\\bA\\,\\bu = \\bC \\boldsymbol{0}$, which has the trivial solution $\\bu = \\boldsymbol{0}$;\n",
    "- the adjoint problem $\\bA^T \\bp = -\\bB^T\\boldsymbol{d}$.\n",
    "2) Each Hessian-vector multiplication in a direction $\\hat{\\bm}$ requires solving\n",
    "- One incremental forward problem $\\bA \\hat{\\bu} = \\bC \\hat{\\bm}$;\n",
    "- One incremental adjoint problem $\\bA^T\\hat{\\bp} = -\\bB^T\\bB \\hat{\\bu}$.\n",
    "\n",
    "\n",
    "\n",
    "## Spectrum of the Hessian\n",
    "\n",
    "We decompose the Hessian $\\bH$ in two components\n",
    "\n",
    "$$ \\bH = \\underbrace{\\bC^T \\bA^{-T}\\bB^T\\bB\\bA^{-1}\\bC}_{\\text{Hessian of the data misfit} \\bH_m} + \\underbrace{\\bR}_{\\text{Hessian of the regularization}}. $$\n",
    "\n",
    "We then consider the generalized eigenproblem\n",
    "\n",
    "$$ \\bH_m \\hat{\\bm}_i = \\lambda_i \\bR \\hat{\\bm}_i$$\n",
    "\n",
    "Note that:\n",
    "1) Eigenvectors $\\hat{\\bm}_i$ corresponding to large eigenvalues ($\\lambda_i \\gg 1$) denotes modes (directions) in parameter space that are strongly informed by the data and are weakly penalized by the regularization.\n",
    "2) Eigenvectors $\\hat{\\bm}_i$ corresponding to small eigenvalues ($\\lambda_i \\ll 1$) denotes modes (directions) in parameter space that are not informed by the data and are strongly penalized by the regularization.\n",
    "\n",
    "For this reason, we can refer to the number of eigenvalues larger than one as the *information dimension* which is always smaller than both the parameter and the data dimensions.\n",
    "\n",
    "Scalable algorithms for the solution of inverse problems should have a cost---measured in terms of forward/adjoint/incremental solves---that is independent of parameter/data dimension and only depends on the *information dimension*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import dolfin as dl\n",
    "import numpy as np\n",
    "\n",
    "from hippylib import *\n",
    "\n",
    "\n",
    "import logging\n",
    "logging.getLogger('FFC').setLevel(logging.WARNING)\n",
    "logging.getLogger('UFL').setLevel(logging.WARNING)\n",
    "dl.set_log_active(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The linear source inversion problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A_varf(u,p):\n",
    "    return k*dl.inner(dl.nabla_grad(u), dl.nabla_grad(p))*dl.dx \\\n",
    "           + c*u*p*dl.dx\n",
    "        \n",
    "def f_varf(p):\n",
    "    return dl.Constant(0.)*p*dl.dx\n",
    "        \n",
    "def C_varf(m,p):\n",
    "           return -m*p*dl.dx\n",
    "    \n",
    "def R_varf(m_trial, m_test, gamma, delta):\n",
    "    return dl.Constant(delta)*m_trial*m_test*dl.dx \\\n",
    "           + dl.Constant(gamma)*dl.inner(dl.grad(m_trial), dl.grad(m_test))*dl.dx\n",
    "\n",
    "def u_boundary(x, on_boundary):\n",
    "    return on_boundary and x[1] < dl.DOLFIN_EPS\n",
    "\n",
    "class Hessian:\n",
    "    def __init__(self, A_solver, At_solver, C,B,R):\n",
    "        self.misfit_only = False\n",
    "        self.A_solver = A_solver\n",
    "        self.At_solver = At_solver\n",
    "        self.C = C\n",
    "        self.B = B\n",
    "        self.R = R\n",
    "        \n",
    "        self.uhat = dl.Vector()\n",
    "        self.C.init_vector(self.uhat,0)\n",
    "        self.fwd_rhs = dl.Vector()\n",
    "        self.C.init_vector(self.fwd_rhs,0)\n",
    "        self.adj_rhs = dl.Vector()\n",
    "        self.C.init_vector(self.adj_rhs,0)\n",
    "        self.phat = dl.Vector()\n",
    "        self.C.init_vector(self.phat,0)\n",
    "        \n",
    "    def init_vector(self, x, dim):\n",
    "        self.R.init_vector(x,dim)\n",
    "        \n",
    "    def mult(self, x, y):\n",
    "        y.zero()\n",
    "        self.C.mult(x, self.fwd_rhs)\n",
    "        self.A_solver.solve(self.uhat, self.fwd_rhs)\n",
    "        self.B.transpmult(self.B*self.uhat, self.adj_rhs)\n",
    "        self.At_solver.solve(self.phat, self.adj_rhs)\n",
    "        self.C.transpmult(self.phat, y)\n",
    "        \n",
    "        if not self.misfit_only:\n",
    "            y.axpy(1., self.R*x)\n",
    "        \n",
    "\n",
    "def solve(nx, ny, targets, gamma, delta, verbose=True):\n",
    "    \n",
    "    rand_gen = Random()\n",
    "    mesh = dl.UnitSquareMesh(nx, ny)\n",
    "    Vh1 = dl.FunctionSpace(mesh, 'Lagrange', 1)\n",
    "    \n",
    "    Vh = [Vh1, Vh1, Vh1]\n",
    "    if verbose:\n",
    "        print(\"Number of dofs: STATE={0}, PARAMETER={1}, ADJOINT={2}\".format(\n",
    "            Vh[STATE].dim(), Vh[PARAMETER].dim(), Vh[ADJOINT].dim()) )\n",
    "\n",
    "\n",
    "    u_bdr = dl.Constant(0.0)\n",
    "    bc = dl.DirichletBC(Vh[STATE], u_bdr, u_boundary)\n",
    "    \n",
    "    u_trial = dl.TrialFunction(Vh[STATE])\n",
    "    u_test  = dl.TestFunction(Vh[STATE])\n",
    "    m_trial = dl.TrialFunction(Vh[PARAMETER])\n",
    "    m_test  = dl.TestFunction(Vh[PARAMETER])\n",
    "    \n",
    "    A,f = dl.assemble_system(A_varf(u_trial, u_test), f_varf(u_test), bcs=bc )\n",
    "    A_solver = dl.PETScLUSolver()\n",
    "    A_solver.set_operator(A)\n",
    "    \n",
    "    At,dummy = dl.assemble_system(dl.adjoint( A_varf(u_trial, u_test) ), dl.Constant(0.)*u_test*dl.dx, bcs=bc )\n",
    "    At_solver = dl.PETScLUSolver()\n",
    "    At_solver.set_operator(At)\n",
    "    \n",
    "    C = dl.assemble(C_varf(m_trial, u_test))\n",
    "    bc.zero(C)\n",
    "    \n",
    "    R = dl.assemble(R_varf(m_trial, m_test, gamma, delta))\n",
    "    R_solver = dl.PETScLUSolver()\n",
    "    R_solver.set_operator(R)\n",
    "\n",
    "    mtrue = dl.interpolate(\n",
    "        dl.Expression('min(0.5,exp(-100*(pow(x[0]-0.35,2) +  pow(x[1]-0.7,2))))',degree=5), Vh[PARAMETER]).vector()\n",
    "    m0 = dl.interpolate(dl.Constant(0.0), Vh[PARAMETER]).vector()\n",
    "     \n",
    "    if verbose:\n",
    "        print( \"Number of observation points: {0}\".format(targets.shape[0]) )\n",
    "    \n",
    "    B = assemblePointwiseObservation(Vh[STATE], targets)\n",
    "               \n",
    "    #Generate synthetic observations\n",
    "    utrue = dl.Function(Vh[STATE]).vector()\n",
    "    A_solver.solve(utrue, -(C*mtrue) )\n",
    "    d = B*utrue\n",
    "    MAX = d.norm(\"linf\")\n",
    "    rel_noise = 0.01\n",
    "    noise_std_dev = rel_noise * MAX\n",
    "    rand_gen.normal_perturb(noise_std_dev, d)\n",
    "\n",
    "    u = dl.Function(Vh[STATE]).vector()\n",
    "    m = m0.copy()\n",
    "    p = dl.Function(Vh[ADJOINT]).vector()\n",
    "    mg = dl.Function(Vh[PARAMETER]).vector()\n",
    "    rhs_adj = dl.Function(Vh[STATE]).vector()\n",
    "    \n",
    "    # Solve forward:\n",
    "    A_solver.solve(u, -(C*m) )\n",
    "    # rhs for adjoint\n",
    "    B.transpmult(d-(B*u), rhs_adj)\n",
    "    # solve adj problem\n",
    "    At_solver.solve(p, rhs_adj)\n",
    "    #gradient\n",
    "    C.transpmult(p, mg)\n",
    "    mg.axpy(1., R*m)\n",
    "   \n",
    "\n",
    "    H = Hessian(A_solver,At_solver,C,B,R)\n",
    "\n",
    "    solver = CGSolverSteihaug()\n",
    "    solver.set_operator(H)\n",
    "    solver.set_preconditioner( R_solver )\n",
    "    solver.parameters[\"print_level\"] = -1\n",
    "    solver.parameters[\"rel_tolerance\"] = 1e-9\n",
    "    solver.solve(m, -mg)\n",
    "\n",
    "    if solver.converged:\n",
    "        if verbose:\n",
    "            print (\"CG converged in \", solver.iter, \" iterations.\")\n",
    "    else:\n",
    "        print( \"CG did not converged.\")\n",
    "        raise\n",
    "\n",
    "    # Solve forward:\n",
    "    A_solver.solve(u, -(C*m) )\n",
    " \n",
    "    if verbose:\n",
    "        plt.figure(figsize=(18,8))\n",
    "        plt.subplot(2, 3, 1)\n",
    "        dl.plot(dl.Function(Vh[PARAMETER], mtrue), title = \"True source\")\n",
    "        plt.subplot(2, 3, 2)\n",
    "        dl.plot(dl.Function(Vh[STATE], utrue), title=\"True state\")\n",
    "        plt.subplot(2, 3, 3)\n",
    "        nb.plot_pts(targets, d,mytitle=\"Observations\")\n",
    "        plt.subplot(2, 3, 4)\n",
    "        dl.plot(dl.Function(Vh[PARAMETER], m), title = \"Reconstructed source\")\n",
    "        plt.subplot(2, 3, 5)\n",
    "        dl.plot(dl.Function(Vh[STATE], u), title=\"Reconstructed state\")\n",
    "        plt.subplot(2, 3, 6)\n",
    "        nb.plot_pts(targets, B*u-d,mytitle=\"Misfit\")\n",
    "        plt.show()\n",
    "\n",
    "    H.misfit_only = True\n",
    "    k_evec = 80\n",
    "    p_evec = 5\n",
    "    if verbose:\n",
    "        print (\"Double Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}.\".format(k_evec,p_evec))\n",
    "    \n",
    "    Omega = MultiVector(m, k_evec+p_evec)\n",
    "    rand_gen.normal(1., Omega)\n",
    "    lmbda, U = doublePassG(H, R, R_solver, Omega, k_evec)\n",
    "\n",
    "    if verbose:\n",
    "        plt.figure()\n",
    "        nb.plot_eigenvalues(lmbda, mytitle=\"Generalized Eigenvalues\")\n",
    "        nb.plot_eigenvectors(Vh[PARAMETER], U, mytitle=\"Eigenvectors\", which=[0,1,2,5,10,15])\n",
    "        plt.show()\n",
    "        \n",
    "    return lmbda, U, Vh[PARAMETER], solver.iter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Solution of the source inversion problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim = 2\n",
    "nx = 32\n",
    "ny = 32\n",
    "\n",
    "ntargets = 256\n",
    "np.random.seed(seed=1)\n",
    "targets = np.random.uniform(0.1,0.9, [ntargets, ndim] )\n",
    "\n",
    "\n",
    "gamma = 1e-5\n",
    "delta = 1e-9\n",
    "\n",
    "k = dl.Constant(1.0)\n",
    "c = dl.Constant(0.1)\n",
    "\n",
    "lmbda, U, Vm, nit = solve(nx,ny, targets, gamma, delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mesh independence of the spectrum of the preconditioned Hessian misfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1e-5\n",
    "delta = 1e-9\n",
    "\n",
    "k = dl.Constant(1.0)\n",
    "c = dl.Constant(0.1)\n",
    "\n",
    "n = [16,32,64]\n",
    "lmbda1, U1, Vm1, niter1 = solve(n[0],n[0], targets, gamma, delta,verbose=False)\n",
    "lmbda2, U2, Vm2, niter2 = solve(n[1],n[1], targets, gamma, delta,verbose=False)\n",
    "lmbda3, U3, Vm3, niter3 = solve(n[2],n[2], targets, gamma, delta,verbose=False)\n",
    "\n",
    "print (\"Number of Iterations: \", niter1, niter2, niter3)\n",
    "plt.figure(figsize=(18,4))\n",
    "nb.plot_eigenvalues(lmbda1, mytitle=\"Eigenvalues Mesh {0} by {1}\".format(n[0],n[0]), subplot_loc=131)\n",
    "plt.ylim([1e-3,1e11])\n",
    "nb.plot_eigenvalues(lmbda2, mytitle=\"Eigenvalues Mesh {0} by {1}\".format(n[1],n[1]), subplot_loc=132)\n",
    "plt.ylim([1e-3,1e11])\n",
    "nb.plot_eigenvalues(lmbda3, mytitle=\"Eigenvalues Mesh {0} by {1}\".format(n[2],n[2]), subplot_loc=133)\n",
    "plt.ylim([1e-3,1e11])\n",
    "\n",
    "nb.plot_eigenvectors(Vm1, U1, mytitle=\"Mesh {0} by {1} Eigenvector\".format(n[0],n[0]), which=[0,1,5])\n",
    "nb.plot_eigenvectors(Vm2, U2, mytitle=\"Mesh {0} by {1} Eigenvector\".format(n[1],n[1]), which=[0,1,5])\n",
    "nb.plot_eigenvectors(Vm3, U3, mytitle=\"Mesh {0} by {1} Eigenvector\".format(n[2],n[2]), which=[0,1,5])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dependence on regularization parameters\n",
    "\n",
    "We solve the problem for different values of the regularization parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = [1e-4, 1e-5, 1e-6]\n",
    "delta = [1e-8, 1e-9, 1e-10]\n",
    "\n",
    "k = dl.Constant(1.0)\n",
    "c = dl.Constant(0.1)\n",
    "\n",
    "lmbda1, U1, Vm1, niter1 = solve(nx,ny, targets, gamma[0], delta[0],verbose=False)\n",
    "lmbda2, U2, Vm2, niter2 = solve(nx,ny, targets, gamma[1], delta[1],verbose=False)\n",
    "lmbda3, U3, Vm3, niter3 = solve(nx,ny, targets, gamma[2], delta[2],verbose=False)\n",
    "\n",
    "print (\"Number of Iterations: \", niter1, niter2, niter3)\n",
    "plt.figure(figsize=(18,4))\n",
    "nb.plot_eigenvalues(lmbda1, mytitle=\"Eigenvalues gamma={0:1.1e}\".format(gamma[0]), subplot_loc=131)\n",
    "plt.ylim([1e-3,1e12])\n",
    "nb.plot_eigenvalues(lmbda2, mytitle=\"Eigenvalues gamma={0:1.1e}\".format(gamma[1]), subplot_loc=132)\n",
    "plt.ylim([1e-3,1e12])\n",
    "nb.plot_eigenvalues(lmbda3, mytitle=\"Eigenvalues gamma={0:1.1e}\".format(gamma[2]), subplot_loc=133)\n",
    "plt.ylim([1e-3,1e12])\n",
    "\n",
    "nb.plot_eigenvectors(Vm1, U1, mytitle=\"gamma={0:1.1e} Eigenvector\".format(gamma[0]), which=[0,1,5])\n",
    "nb.plot_eigenvectors(Vm2, U2, mytitle=\"gamma={0:1.1e} Eigenvector\".format(gamma[1]), which=[0,1,5])\n",
    "nb.plot_eigenvectors(Vm3, U3, mytitle=\"gamma={0:1.1e} Eigenvector\".format(gamma[2]), which=[0,1,5])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dependence on the PDE coefficients\n",
    "\n",
    "Assume a constant reaction term $c = 0.1$, and we consider different values for the diffusivity coefficient $k$.\n",
    "\n",
    "The smaller the value of $k$ the slower the decay in the spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1e-5\n",
    "delta = 1e-9\n",
    "\n",
    "k = dl.Constant(1.0)\n",
    "c = dl.Constant(0.1)\n",
    "\n",
    "lmbda1, U1, Vm1, niter1 = solve(nx,ny, targets, gamma, delta,verbose=False)\n",
    "k = dl.Constant(0.1)\n",
    "lmbda2, U2, Vm2, niter2 = solve(nx,ny, targets, gamma, delta,verbose=False)\n",
    "k = dl.Constant(0.01)\n",
    "lmbda3, U3, Vm3, niter3 = solve(nx,ny, targets, gamma, delta,verbose=False)\n",
    "\n",
    "print (\"Number of Iterations: \", niter1, niter2, niter3)\n",
    "plt.figure(figsize=(18,4))\n",
    "nb.plot_eigenvalues(lmbda1, mytitle=\"Eigenvalues k=1.0\", subplot_loc=131)\n",
    "plt.ylim([1e-2,1e14])\n",
    "nb.plot_eigenvalues(lmbda2, mytitle=\"Eigenvalues k=0.1\", subplot_loc=132)\n",
    "plt.ylim([1e-2,1e14])\n",
    "nb.plot_eigenvalues(lmbda3, mytitle=\"Eigenvalues k=0.01\", subplot_loc=133)\n",
    "plt.ylim([1e-2,1e14])\n",
    "\n",
    "nb.plot_eigenvectors(Vm1, U1, mytitle=\"k=1. Eigenvector\", which=[0,1,5])\n",
    "nb.plot_eigenvectors(Vm2, U2, mytitle=\"k=0.1 Eigenvector\", which=[0,1,5])\n",
    "nb.plot_eigenvectors(Vm3, U3, mytitle=\"k=0.01 Eigenvector\", which=[0,1,5])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Dependence on the number of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntargets = [16, 64, 256]\n",
    "\n",
    "gamma = 1e-5\n",
    "delta = 1e-9\n",
    "\n",
    "k = dl.Constant(0.1)\n",
    "c = dl.Constant(0.1)\n",
    "\n",
    "lmbda1, U1, Vm1, niter1 = solve(nx,ny, targets[0:ntargets[0],:], gamma, delta,verbose=False)\n",
    "lmbda2, U2, Vm2, niter2 = solve(nx,ny, targets[0:ntargets[1],:], gamma, delta,verbose=False)\n",
    "lmbda3, U3, Vm3, niter3 = solve(nx,ny, targets[0:ntargets[2],:], gamma, delta,verbose=False)\n",
    "\n",
    "print (\"Number of Iterations: \", niter1, niter2, niter3)\n",
    "plt.figure(figsize=(18,4))\n",
    "nb.plot_eigenvalues(lmbda1, mytitle=\"Eigenvalues ntargets={0}\".format(ntargets[0]), subplot_loc=131)\n",
    "plt.ylim([1e-6,1e12])\n",
    "nb.plot_eigenvalues(lmbda2, mytitle=\"Eigenvalues ntargets={0}\".format(ntargets[1]), subplot_loc=132)\n",
    "plt.ylim([1e-6,1e12])\n",
    "nb.plot_eigenvalues(lmbda3, mytitle=\"Eigenvalues ntargets={0}\".format(ntargets[2]), subplot_loc=133)\n",
    "plt.ylim([1e-6,1e12])\n",
    "\n",
    "nb.plot_eigenvectors(Vm1, U1, mytitle=\"ntargets={0} Eigenvector\".format(ntargets[0]), which=[0,1,5])\n",
    "nb.plot_eigenvectors(Vm2, U2, mytitle=\"ntargets={0} Eigenvector\".format(ntargets[1]), which=[0,1,5])\n",
    "nb.plot_eigenvectors(Vm3, U3, mytitle=\"ntargets={0} Eigenvector\".format(ntargets[2]), which=[0,1,5])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Copyright &copy; 2019-2020, Washington University in St. Louis.\n",
    "\n",
    "All Rights reserved.\n",
    "See file COPYRIGHT for details.\n",
    "\n",
    "This file is part of **cmis_labs**, the teaching material for  ESE 5932 *Computational Methods for Imaging Science* at Washington University in St. Louis. Please see [https://uvilla.github.io/cmis_labs](https://uvilla.github.io/cmis_labs) for more information and source code availability.\n",
    "\n",
    "We would like to acknowledge the Extreme Science and Engineering Discovery Environment (XSEDE), which is supported by National Science Foundation grant number ACI-1548562, for providing cloud computing resources (Jetstream) for this course through allocation TG-SEE190001."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
